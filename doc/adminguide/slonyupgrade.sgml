<!--  -->
<sect1 id="slonyupgrade">
<title> &slony1; Upgrade </title>
<indexterm><primary>upgrading &slony1; to a newer version</primary></indexterm>

<para> When upgrading &slony1;, the installation on all nodes in a
cluster must be upgraded at once, using the &lslonik;
command <xref linkend="stmtupdatefunctions">.</para>

<para> While this requires temporarily stopping replication, it does
not forcibly require an outage for applications that submit
updates. </para>

<para>The proper upgrade procedure is thus:</para>
<itemizedlist>
<listitem><para> Stop the &lslon; processes on all nodes.
(<emphasis>e.g.</emphasis> - old version of &lslon;)</para></listitem>
<listitem><para> Install the new version of &lslon; software on all
nodes.</para></listitem>
<listitem><para> Execute a &lslonik; script containing the
command <command>update functions (id = [whatever]);</command> for
each node in the cluster.</para>
<note><para>Remember that your slonik upgrade script like all other 
slonik scripts must contain the proper preamble commands to function.
</para></note></listitem>
<listitem><para> Start all slons.  </para> </listitem>
</itemizedlist>

<para> The overall operation is relatively safe: If there is any
mismatch between component versions, the &lslon; will refuse to start
up, which provides protection against corruption. </para>

<para> You need to be sure that the C library containing SPI trigger
functions has been copied into place in the &postgres; build.  There
are multiple possible approaches to this:</para>

<para> The trickiest part of this is ensuring that the C library
containing SPI functions is copied into place in the &postgres; build;
the easiest and safest way to handle this is to have two separate
&postgres; builds, one for each &slony1; version, where the postmaster
is shut down and then restarted against the <quote>new</quote> build;
that approach requires a brief database outage on each node.</para>

<para> While that approach has been found to be easier and safer,
nothing prevents one from carefully copying &slony1; components for
the new version into place to overwrite the old version as
the <quote>install</quote> step.  That might <emphasis>not</emphasis>
work on <trademark>Windows</trademark> if it locks library files that
are in use.</para>

<variablelist>

<varlistentry><term>Run <command>make install</command> to install new
&slony1; components on top of the old</term>

<listitem><para>If you build &slony1; on the same system on which it
is to be deployed, and build from sources, overwriting the old with
the new is as easy as <command>make install</command>.  There is no
need to restart a database backend; just to stop &lslon; processes,
run the <command>UPDATE FUNCTIONS</command> script, and start new
&lslon; processes.</para>

<para> Unfortunately, this approach requires having a build
environment on the same host as the deployment.  That may not be
consistent with efforts to use common &postgres; and &slony1; binaries
across a set of nodes. </para>
</listitem></varlistentry>

<varlistentry><term>Create a new &postgres; and &slony1; build</term>

<listitem><para>With this approach, the old &postgres; build with old
&slony1; components persists after switching to a new &postgres; build
with new &slony1; components. In order to switch to the new &slony1;
build, you need to restart the
&postgres; <command>postmaster</command>, therefore interrupting
applications, in order to get it to be aware of the location of the
new components. </para></listitem></varlistentry>

</variablelist>

<sect2> <title> TABLE ADD KEY issue in &slony1; 2.0 </title> 

<para> Usually, upgrades between &slony1; versions have required no
special attention to the condition of the existing replica.  That is,
you fairly much merely need to stop &lslon;s, put new binaries in
place, run <xref linkend="stmtupdatefunctions"> against each node, and
restart &lslon;s.  Schema changes have been internal to the cluster
schema, and <xref linkend="stmtupdatefunctions"> has been capable to
make all of the needed alterations.  With version 2, this changes, if
there are tables that used <xref linkend="stmttableaddkey">.  Version
2 does not support the <quote>extra</quote> column, and
<quote>fixing</quote> the schema to have a proper primary key is not
within the scope of what <xref linkend="stmtupdatefunctions"> can
perform.  </para>

<para> When upgrading from versions 1.0.x, 1.1.x, or 1.2.x to version
2, it will be necessary to have already eliminated any such
&slony1;-managed primary keys. </para>

<para> One may identify the tables affected via the following SQL
query: <command> select n.nspname, c.relname from pg_class c,
pg_namespace n where c.oid in (select attrelid from pg_attribute where
attname like '_Slony-I_%rowID' and not attisdropped) and reltype &lt;&gt; 0
and n.oid = c.relnamespace order by n.nspname, c.relname; </command>
</para>

<para> The simplest approach that may be taken to rectify the
<quote>broken</quote> state of such tables is as follows: </para>

<itemizedlist>

<listitem><para> Drop the table from replication using the &lslonik;
command <xref linkend="stmtsetdroptable">. </para>

<para> This does <emphasis>not</emphasis> drop out the
&slony1;-generated column. </para>
</listitem>

<listitem><para> On each node, run an SQL script to alter the table,
dropping the extra column.</para> <para> <command> alter table
whatever drop column "_Slony-I_cluster-rowID";</command> </para>

<para> This needs to be run individually against each node.  Depending
on your preferences, you might wish to use <xref
linkend="stmtddlscript"> to do this. </para>

<para> If the table is a heavily updated one, it is worth observing
that this alteration will require acquiring an exclusive lock on the
table.  It will not hold this lock for terribly long; dropping the
column should be quite a rapid operation as all it does internally is
to mark the column as being dropped; it <emphasis>does not</emphasis>
require rewriting the entire contents of the table.  Tuples that have
values in that column will continue to have that value; new tuples
will leave it NULL, and queries will ignore the column.  Space for
those columns will get reclaimed as tuples get updated.  </para>

<para> Note that at this point in the process, this table is not being
replicated.  If a failure takes place, replication is not, at this
point, providing protection on this table.  This is unfortunate but
unavoidable. </para>
</listitem>

<listitem><para> Make sure the table has a legitimate candidate for
primary key, some set of NOT NULL, UNIQUE columns.  </para>

<para> The possible variations to this are the reason that the
developers have made no effort to try to assist automation of
this.</para></listitem>
</itemizedlist>

<itemizedlist>

<listitem><para> If the table is a small one, it may be perfectly
reasonable to do alterations (note that they must be applied to
<emphasis>every node</emphasis>!) to add a new column, assign it via a
new sequence, and then declare it to be a primary key.  </para>

<para> If there are only a few tuples, this should take a fraction of
a second, and, with luck, be unnoticeable to a running
application. </para>

<para> Even if the table is fairly large, if it is not frequently
accessed by the application, the locking of the table that takes place
when you run <command>ALTER TABLE</command> may not cause much
inconvenience. </para></listitem>

<listitem> <para> If the table is a large one, and is vital to and
heavily accessed by the application, then it may be necessary to take
an application outage in order to accomplish the alterations, leaving
you necessarily somewhat vulnerable until the process is
complete. </para>

<para> If it is troublesome to take outages, then the upgrade to
&slony1; version 2 may take some planning... </para>
</listitem>

</itemizedlist>

<itemizedlist>

<listitem><para> Create a new replication set (<xref
linkend="stmtcreateset">) and re-add the table to that set (<xref
linkend="stmtsetaddtable">).  </para>

<para> If there are multiple tables, they may be handled via a single
replication set.</para>
</listitem>

<listitem><para> Subscribe the set (<xref linkend="stmtsubscribeset">)
on all the nodes desired. </para> </listitem>

<listitem><para> Once subscriptions are complete, merge the set(s) in,
if desired (<xref linkend="stmtmergeset">). </para> </listitem>

</itemizedlist>

<para> This approach should be fine for tables that are relatively
small, or infrequently used.  If, on the other hand, the table is
large and heavily used, another approach may prove necessary, namely
to create your own sequence, and <quote>promote</quote> the formerly
&slony1;-generated column into a <quote>real</quote> column in your
database schema.  An outline of the steps is as follows: </para>

<itemizedlist>

<listitem><para> Add a sequence that assigns values to the
column. </para>

<para> Setup steps will include SQL <command>CREATE
SEQUENCE</command>, SQL <command>SELECT SETVAL()</command> (to set the
value of the sequence high enough to reflect values used in the
table), Slonik <xref linkend="stmtcreateset"> (to create a set to
assign the sequence to), Slonik <xref linkend="stmtsetaddsequence">
(to assign the sequence to the set), Slonik <xref
linkend="stmtsubscribeset"> (to set up subscriptions to the new
set)</para>
</listitem>

<listitem><para> Attach the sequence to the column on the
table. </para>

<para> This involves <command>ALTER TABLE ALTER COLUMN</command>,
which must be submitted via the Slonik command <xref
linkend="stmtddlscript">. </para>
</listitem>

<listitem><para> Rename the column
<envar>_Slony-I_@CLUSTERNAME@_rowID</envar> so that &slony1; won't
consider it to be under its control.</para>

<para> This involves <command>ALTER TABLE ALTER COLUMN</command>,
which must be submitted via the Slonik command <xref
linkend="stmtddlscript">. </para>

<para> Note that these two alterations might be accomplished via the
same <xref linkend="stmtddlscript"> request. </para>
</listitem>

</itemizedlist>

</sect2>

<sect2> <title> New Trigger Handling in &slony1; Version 2 </title>

<para> One of the major changes to &slony1; is that enabling/disabling
of triggers and rules now takes place as plain SQL, supported by
&postgres; 8.3+, rather than via <quote>hacking</quote> on the system
catalog. </para>

<para> As a result, &slony1; users should be aware of the &postgres;
syntax for <command>ALTER TABLE</command>, as that is how they can
accomplish what was formerly accomplished via <xref
linkend="stmtstoretrigger"> and <xref linkend="stmtdroptrigger">. </para>

</sect2>

<sect2 id="upgrade20"> <title> Upgrading to &slony1; version 2 </title>

<para> The version 2 branch is <emphasis>substantially</emphasis>
different from earlier releases, dropping support for versions of
&postgres; prior to 8.3, as in version 8.3, support for a
<quote>session replication role</quote> was added, thereby eliminating
the need for system catalog hacks as well as the
not-entirely-well-supported <envar>xxid</envar> data type. </para>

<para> As a result of the replacement of the <envar>xxid</envar> type
with a (native-to-8.3) &postgres; transaction XID type, the &lslonik;
command <xref linkend="stmtupdatefunctions"> is quite inadequate to
the process of upgrading earlier versions of &slony1; to version
2.</para>

<para> In version 2.0.2, we have added a new option to <xref
linkend="stmtsubscribeset">, <command>OMIT COPY</command>, which
allows taking an alternative approach to upgrade which amounts to:</para>

<itemizedlist>
<listitem><para> Uninstall old version of &slony1; </para>
<para> When &slony1; uninstalls itself, catalog corruptions are fixed back up.</para> </listitem>
<listitem><para> Install &slony1; version 2 </para></listitem>
<listitem><para> Resubscribe, with <command>OMIT COPY</command></para></listitem>
</itemizedlist>

<warning><para> There is a large <quote>foot gun</quote> here: during
part of the process, &slony1; is not installed in any form, and if an
application updates one or another of the databases, the
resubscription, omitting copying data, will be left with data
<emphasis>out of sync.</emphasis> </para>

<para> The administrator <emphasis>must take care</emphasis>; &slony1;
has no way to help ensure the integrity of the data during this
process.</para>
</warning>

<para> The following process is suggested to help make the upgrade
process as safe as possible, given the above risks. </para>

<itemizedlist>

<listitem><para> Use <xref linkend="slonikconfdump"> to generate a
&lslonik; script to recreate the replication cluster.  </para>

<para> Be sure to verify the <xref linkend="admconninfo"> statements,
as the values are pulled are drawn from the PATH configuration, which
may not necessarily be suitable for running &lslonik;. </para>

<para> This step may be done before the application outage. </para>
</listitem>

<listitem><para> Determine what triggers have <xref
linkend="stmtstoretrigger"> configuration on subscriber nodes.
</para>

<para> As discussed in <xref linkend="triggers">, the handling has
fundamentally changed between &slony1; 1.2 and 2.0. </para>

<para> Generally speaking, what needs to happen is to query
<envar>sl_table</envar> on each node, and, for any triggers found in
<envar>sl_table</envar>, it is likely to be appropriate to set up a
script indicating either <command>ENABLE REPLICA TRIGGER</command> or
<command>ENABLE ALWAYS TRIGGER</command> for these triggers.</para>

<para> This step may be done before the application outage. </para>
</listitem>

<listitem><para> Begin an application outage during which updates should no longer be applied to the database. </para> </listitem>

<listitem><para> To ensure that applications cease to make changes, it would be appropriate to lock them out via modifications to <filename>pg_hba.conf</filename> </para> </listitem>

<listitem><para> Ensure replication is entirely caught up, via examination of the <envar>sl_status</envar> view, and any application data that may seem appropriate. </para> </listitem>

<listitem><para> Shut down &lslon; processes. </para> </listitem>

<listitem><para> Uninstall the old version of &slony1; from the database. </para> 

<para> This involves running a &lslonik; script that runs <xref
linkend="stmtuninstallnode"> against each node in the cluster. </para>

</listitem>

<listitem><para> Ensure new &slony1; binaries are in place. </para> 

<para> A convenient way to handle this is to have old and new in different directories alongside two &postgres; builds, stop the <application>postmaster</application>, repoint to the new directory, and restart the <application>postmaster</application>. </para>
</listitem>

<listitem><para> Run the script that reconfigures replication as generated earlier.  </para> 

<para> This script should probably be split into two portions to be run separately:</para> 
<itemizedlist>
<listitem><para> Firstly, set up nodes, paths, sets, and such </para> </listitem>
<listitem><para> At this point, start up &lslon; processes </para> </listitem>
<listitem><para> Then, run the portion which runs <xref linkend="stmtsubscribeset"> </para> </listitem>
</itemizedlist>

<para> Splitting the <xref linkend="slonikconfdump"> script as described above is left as an exercise for the reader.</para>
</listitem>

<listitem><para> If there were triggers that needed to be activated on subscriber nodes, this is the time to activate them. </para> </listitem>
<listitem><para> At this point, the cluster should be back up and running, ready to be reconfigured so that applications may access it again.  </para> </listitem>

</itemizedlist>

</sect2>
</sect1>
<!-- Keep this comment at the end of the file
Local variables:
mode:sgml
sgml-omittag:nil
sgml-shorttag:t
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:1
sgml-indent-data:t
sgml-parent-document:"slony.sgml"
sgml-exposed-tags:nil
sgml-local-catalogs:("/usr/lib/sgml/catalog")
sgml-local-ecat-files:nil
End:
-->
